{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/homerabbitsky/HelloAI/blob/main/Theano2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!cat ~/.keras/keras.json\n",
        "!git clone \"https://github.com/kartoone/nn3\"\n",
        "!cat nn3/keras.json > ~/.keras/keras.json\n",
        "!pip uninstall -y keras\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow==2.2\n",
        "!pip install keras==2.2.4\n",
        "!pip install pydot-ng\n",
        "!pip install theano==0.8\n",
        "%cd nn3/src\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zufu4wflP581",
        "outputId": "8104fc1a-80cc-4bc7-8563-cac6fdf5a011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nn3'...\n",
            "remote: Enumerating objects: 188, done.\u001b[K\n",
            "remote: Counting objects: 100% (79/79), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 188 (delta 42), reused 29 (delta 12), pack-reused 109\u001b[K\n",
            "Receiving objects: 100% (188/188), 94.38 MiB | 10.36 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Updating files: 100% (108/108), done.\n",
            "Found existing installation: keras 2.11.0\n",
            "Uninstalling keras-2.11.0:\n",
            "  Successfully uninstalled keras-2.11.0\n",
            "Found existing installation: tensorflow 2.11.0\n",
            "Uninstalling tensorflow-2.11.0:\n",
            "  Successfully uninstalled tensorflow-2.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.2\n",
            "  Downloading tensorflow-2.2.0-cp38-cp38-manylinux2010_x86_64.whl (516.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (0.38.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (2.2.0)\n",
            "Collecting keras-preprocessing>=1.1.0\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.51.1)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.6/454.6 KB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (3.19.6)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (3.3.0)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.4.1\n",
            "  Downloading scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl (26.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.21.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2) (1.14.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (57.4.0)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 KB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.25.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.2.8)\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.13.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, scipy, keras-preprocessing, h5py, gast, cachetools, google-auth, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.11.0\n",
            "    Uninstalling tensorflow-estimator-2.11.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.11.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.0\n",
            "    Uninstalling cachetools-5.3.0:\n",
            "      Successfully uninstalled cachetools-5.3.0\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.16.0\n",
            "    Uninstalling google-auth-2.16.0:\n",
            "      Successfully uninstalled google-auth-2.16.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.11.2\n",
            "    Uninstalling tensorboard-2.11.2:\n",
            "      Successfully uninstalled tensorboard-2.11.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-vis 0.4.1 requires keras, which is not installed.\n",
            "xarray-einstats 0.5.1 requires scipy>=1.6, but you have scipy 1.4.1 which is incompatible.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
            "jaxlib 0.3.25+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "jax 0.3.25 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "google-api-core 2.11.0 requires google-auth<3.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cachetools-4.2.4 gast-0.3.3 google-auth-1.35.0 h5py-2.10.0 keras-preprocessing-1.1.2 scipy-1.4.1 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.2.4\n",
            "  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.5/312.5 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (6.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (1.21.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.8/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Installing collected packages: keras-applications, keras\n",
            "Successfully installed keras-2.2.4 keras-applications-1.0.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydot-ng in /usr/local/lib/python3.8/dist-packages (2.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from pydot-ng) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting theano==0.8\n",
            "  Downloading Theano-0.8.0.zip (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from theano==0.8) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.11 in /usr/local/lib/python3.8/dist-packages (from theano==0.8) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from theano==0.8) (1.15.0)\n",
            "Building wheels for collected packages: theano\n",
            "  Building wheel for theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for theano: filename=Theano-0.8.0-py3-none-any.whl size=2722137 sha256=aaf06080276422bdbb16987eaa54ad4b5e203dc58a29c87896759bca86783a8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/fa/00/251dbd5e561228175439939ac67c8c17484f615a245c2479d8\n",
            "Successfully built theano\n",
            "Installing collected packages: theano\n",
            "Successfully installed theano-0.8.0\n",
            "/content/nn3/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ~/.keras/keras.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apRaTZgACf0m",
        "outputId": "6599619d-e700-4026-c2ab-b487117fe7f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"floatx\": \"float32\",\n",
            "    \"epsilon\": 1e-07,\n",
            "    \"backend\": \"theano\",\n",
            "    \"image_dim_ordering\": \"th\",\n",
            "    \"image_data_format\": \"channels_first\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nn3/src\n",
        "\n",
        "import network3\n",
        "from network3 import Network\n",
        "from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
        "training_data, validation_data, test_data = network3.load_data_shared()\n",
        "mini_batch_size = 10\n",
        "net = Network([\n",
        "        FullyConnectedLayer(n_in=784, n_out=100),\n",
        "        SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
        "net.SGD(training_data, 60, mini_batch_size, 0.1, \n",
        "            validation_data, test_data)\n"
      ],
      "metadata": {
        "id": "DgOjEgKjVp0o",
        "outputId": "62b41487-374b-48f3-e006-52fb902a2f80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'nn3/src'\n",
            "/content/nn3/src\n",
            "Trying to run under a GPU.  If this is not desired, then modify network3.py\n",
            "to set the GPU flag to False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
            "WARNING:theano.tensor.blas:We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training mini-batch number 0\n",
            "Training mini-batch number 1000\n",
            "Training mini-batch number 2000\n",
            "Training mini-batch number 3000\n",
            "Training mini-batch number 4000\n",
            "Epoch 0: validation accuracy 0.9253000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9207000000000002\n",
            "Training mini-batch number 5000\n",
            "Training mini-batch number 6000\n",
            "Training mini-batch number 7000\n",
            "Training mini-batch number 8000\n",
            "Training mini-batch number 9000\n",
            "Epoch 1: validation accuracy 0.9467000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9436000000000001\n",
            "Training mini-batch number 10000\n",
            "Training mini-batch number 11000\n",
            "Training mini-batch number 12000\n",
            "Training mini-batch number 13000\n",
            "Training mini-batch number 14000\n",
            "Epoch 2: validation accuracy 0.9584000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9536000000000001\n",
            "Training mini-batch number 15000\n",
            "Training mini-batch number 16000\n",
            "Training mini-batch number 17000\n",
            "Training mini-batch number 18000\n",
            "Training mini-batch number 19000\n",
            "Epoch 3: validation accuracy 0.9637\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9590000000000001\n",
            "Training mini-batch number 20000\n",
            "Training mini-batch number 21000\n",
            "Training mini-batch number 22000\n",
            "Training mini-batch number 23000\n",
            "Training mini-batch number 24000\n",
            "Epoch 4: validation accuracy 0.9671000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9632999999999999\n",
            "Training mini-batch number 25000\n",
            "Training mini-batch number 26000\n",
            "Training mini-batch number 27000\n",
            "Training mini-batch number 28000\n",
            "Training mini-batch number 29000\n",
            "Epoch 5: validation accuracy 0.9693\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9661000000000002\n",
            "Training mini-batch number 30000\n",
            "Training mini-batch number 31000\n",
            "Training mini-batch number 32000\n",
            "Training mini-batch number 33000\n",
            "Training mini-batch number 34000\n",
            "Epoch 6: validation accuracy 0.9703\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9685\n",
            "Training mini-batch number 35000\n",
            "Training mini-batch number 36000\n",
            "Training mini-batch number 37000\n",
            "Training mini-batch number 38000\n",
            "Training mini-batch number 39000\n",
            "Epoch 7: validation accuracy 0.9709000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9698000000000002\n",
            "Training mini-batch number 40000\n",
            "Training mini-batch number 41000\n",
            "Training mini-batch number 42000\n",
            "Training mini-batch number 43000\n",
            "Training mini-batch number 44000\n",
            "Epoch 8: validation accuracy 0.9714\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9703\n",
            "Training mini-batch number 45000\n",
            "Training mini-batch number 46000\n",
            "Training mini-batch number 47000\n",
            "Training mini-batch number 48000\n",
            "Training mini-batch number 49000\n",
            "Epoch 9: validation accuracy 0.9717\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9713\n",
            "Training mini-batch number 50000\n",
            "Training mini-batch number 51000\n",
            "Training mini-batch number 52000\n",
            "Training mini-batch number 53000\n",
            "Training mini-batch number 54000\n",
            "Epoch 10: validation accuracy 0.9724\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9725000000000001\n",
            "Training mini-batch number 55000\n",
            "Training mini-batch number 56000\n",
            "Training mini-batch number 57000\n",
            "Training mini-batch number 58000\n",
            "Training mini-batch number 59000\n",
            "Epoch 11: validation accuracy 0.9725\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9732000000000001\n",
            "Training mini-batch number 60000\n",
            "Training mini-batch number 61000\n",
            "Training mini-batch number 62000\n",
            "Training mini-batch number 63000\n",
            "Training mini-batch number 64000\n",
            "Epoch 12: validation accuracy 0.9731000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9735\n",
            "Training mini-batch number 65000\n",
            "Training mini-batch number 66000\n",
            "Training mini-batch number 67000\n",
            "Training mini-batch number 68000\n",
            "Training mini-batch number 69000\n",
            "Epoch 13: validation accuracy 0.9731000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9742999999999999\n",
            "Training mini-batch number 70000\n",
            "Training mini-batch number 71000\n",
            "Training mini-batch number 72000\n",
            "Training mini-batch number 73000\n",
            "Training mini-batch number 74000\n",
            "Epoch 14: validation accuracy 0.9734\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9749000000000001\n",
            "Training mini-batch number 75000\n",
            "Training mini-batch number 76000\n",
            "Training mini-batch number 77000\n",
            "Training mini-batch number 78000\n",
            "Training mini-batch number 79000\n",
            "Epoch 15: validation accuracy 0.973\n",
            "Training mini-batch number 80000\n",
            "Training mini-batch number 81000\n",
            "Training mini-batch number 82000\n",
            "Training mini-batch number 83000\n",
            "Training mini-batch number 84000\n",
            "Epoch 16: validation accuracy 0.9735\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9753000000000001\n",
            "Training mini-batch number 85000\n",
            "Training mini-batch number 86000\n",
            "Training mini-batch number 87000\n",
            "Training mini-batch number 88000\n",
            "Training mini-batch number 89000\n",
            "Epoch 17: validation accuracy 0.9736000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.975\n",
            "Training mini-batch number 90000\n",
            "Training mini-batch number 91000\n",
            "Training mini-batch number 92000\n",
            "Training mini-batch number 93000\n",
            "Training mini-batch number 94000\n",
            "Epoch 18: validation accuracy 0.9739000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9751000000000002\n",
            "Training mini-batch number 95000\n",
            "Training mini-batch number 96000\n",
            "Training mini-batch number 97000\n",
            "Training mini-batch number 98000\n",
            "Training mini-batch number 99000\n",
            "Epoch 19: validation accuracy 0.9746\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9756000000000001\n",
            "Training mini-batch number 100000\n",
            "Training mini-batch number 101000\n",
            "Training mini-batch number 102000\n",
            "Training mini-batch number 103000\n",
            "Training mini-batch number 104000\n",
            "Epoch 20: validation accuracy 0.9748000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9755\n",
            "Training mini-batch number 105000\n",
            "Training mini-batch number 106000\n",
            "Training mini-batch number 107000\n",
            "Training mini-batch number 108000\n",
            "Training mini-batch number 109000\n",
            "Epoch 21: validation accuracy 0.9753000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9753000000000001\n",
            "Training mini-batch number 110000\n",
            "Training mini-batch number 111000\n",
            "Training mini-batch number 112000\n",
            "Training mini-batch number 113000\n",
            "Training mini-batch number 114000\n",
            "Epoch 22: validation accuracy 0.9751000000000001\n",
            "Training mini-batch number 115000\n",
            "Training mini-batch number 116000\n",
            "Training mini-batch number 117000\n",
            "Training mini-batch number 118000\n",
            "Training mini-batch number 119000\n",
            "Epoch 23: validation accuracy 0.9753000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9758000000000001\n",
            "Training mini-batch number 120000\n",
            "Training mini-batch number 121000\n",
            "Training mini-batch number 122000\n",
            "Training mini-batch number 123000\n",
            "Training mini-batch number 124000\n",
            "Epoch 24: validation accuracy 0.9754\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9756000000000001\n",
            "Training mini-batch number 125000\n",
            "Training mini-batch number 126000\n",
            "Training mini-batch number 127000\n",
            "Training mini-batch number 128000\n",
            "Training mini-batch number 129000\n",
            "Epoch 25: validation accuracy 0.9754\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9759000000000001\n",
            "Training mini-batch number 130000\n",
            "Training mini-batch number 131000\n",
            "Training mini-batch number 132000\n",
            "Training mini-batch number 133000\n",
            "Training mini-batch number 134000\n",
            "Epoch 26: validation accuracy 0.9757\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9762000000000001\n",
            "Training mini-batch number 135000\n",
            "Training mini-batch number 136000\n",
            "Training mini-batch number 137000\n",
            "Training mini-batch number 138000\n",
            "Training mini-batch number 139000\n",
            "Epoch 27: validation accuracy 0.9759000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.976\n",
            "Training mini-batch number 140000\n",
            "Training mini-batch number 141000\n",
            "Training mini-batch number 142000\n",
            "Training mini-batch number 143000\n",
            "Training mini-batch number 144000\n",
            "Epoch 28: validation accuracy 0.9764\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.976\n",
            "Training mini-batch number 145000\n",
            "Training mini-batch number 146000\n",
            "Training mini-batch number 147000\n",
            "Training mini-batch number 148000\n",
            "Training mini-batch number 149000\n",
            "Epoch 29: validation accuracy 0.9766\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9762000000000001\n",
            "Training mini-batch number 150000\n",
            "Training mini-batch number 151000\n",
            "Training mini-batch number 152000\n",
            "Training mini-batch number 153000\n",
            "Training mini-batch number 154000\n",
            "Epoch 30: validation accuracy 0.9768000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9762000000000001\n",
            "Training mini-batch number 155000\n",
            "Training mini-batch number 156000\n",
            "Training mini-batch number 157000\n",
            "Training mini-batch number 158000\n",
            "Training mini-batch number 159000\n",
            "Epoch 31: validation accuracy 0.9771000000000002\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9763000000000001\n",
            "Training mini-batch number 160000\n",
            "Training mini-batch number 161000\n",
            "Training mini-batch number 162000\n",
            "Training mini-batch number 163000\n",
            "Training mini-batch number 164000\n",
            "Epoch 32: validation accuracy 0.9773000000000002\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9762000000000001\n",
            "Training mini-batch number 165000\n",
            "Training mini-batch number 166000\n",
            "Training mini-batch number 167000\n",
            "Training mini-batch number 168000\n",
            "Training mini-batch number 169000\n",
            "Epoch 33: validation accuracy 0.9775000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9766000000000001\n",
            "Training mini-batch number 170000\n",
            "Training mini-batch number 171000\n",
            "Training mini-batch number 172000\n",
            "Training mini-batch number 173000\n",
            "Training mini-batch number 174000\n",
            "Epoch 34: validation accuracy 0.9775000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9770000000000001\n",
            "Training mini-batch number 175000\n",
            "Training mini-batch number 176000\n",
            "Training mini-batch number 177000\n",
            "Training mini-batch number 178000\n",
            "Training mini-batch number 179000\n",
            "Epoch 35: validation accuracy 0.9775\n",
            "Training mini-batch number 180000\n",
            "Training mini-batch number 181000\n",
            "Training mini-batch number 182000\n",
            "Training mini-batch number 183000\n",
            "Training mini-batch number 184000\n",
            "Epoch 36: validation accuracy 0.9776000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.977\n",
            "Training mini-batch number 185000\n",
            "Training mini-batch number 186000\n",
            "Training mini-batch number 187000\n",
            "Training mini-batch number 188000\n",
            "Training mini-batch number 189000\n",
            "Epoch 37: validation accuracy 0.9777\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.9769000000000001\n",
            "Training mini-batch number 190000\n",
            "Training mini-batch number 191000\n",
            "Training mini-batch number 192000\n",
            "Training mini-batch number 193000\n",
            "Training mini-batch number 194000\n",
            "Epoch 38: validation accuracy 0.9778000000000001\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 0.977\n",
            "Training mini-batch number 195000\n",
            "Training mini-batch number 196000\n",
            "Training mini-batch number 197000\n",
            "Training mini-batch number 198000\n",
            "Training mini-batch number 199000\n",
            "Epoch 39: validation accuracy 0.9777\n",
            "Training mini-batch number 200000\n",
            "Training mini-batch number 201000\n",
            "Training mini-batch number 202000\n",
            "Training mini-batch number 203000\n",
            "Training mini-batch number 204000\n",
            "Epoch 40: validation accuracy 0.9776000000000001\n",
            "Training mini-batch number 205000\n",
            "Training mini-batch number 206000\n",
            "Training mini-batch number 207000\n",
            "Training mini-batch number 208000\n",
            "Training mini-batch number 209000\n",
            "Epoch 41: validation accuracy 0.9777\n",
            "Training mini-batch number 210000\n",
            "Training mini-batch number 211000\n",
            "Training mini-batch number 212000\n",
            "Training mini-batch number 213000\n",
            "Training mini-batch number 214000\n",
            "Epoch 42: validation accuracy 0.9775\n",
            "Training mini-batch number 215000\n",
            "Training mini-batch number 216000\n",
            "Training mini-batch number 217000\n",
            "Training mini-batch number 218000\n",
            "Training mini-batch number 219000\n",
            "Epoch 43: validation accuracy 0.9775\n",
            "Training mini-batch number 220000\n",
            "Training mini-batch number 221000\n",
            "Training mini-batch number 222000\n",
            "Training mini-batch number 223000\n",
            "Training mini-batch number 224000\n",
            "Epoch 44: validation accuracy 0.9774\n",
            "Training mini-batch number 225000\n",
            "Training mini-batch number 226000\n",
            "Training mini-batch number 227000\n",
            "Training mini-batch number 228000\n",
            "Training mini-batch number 229000\n",
            "Epoch 45: validation accuracy 0.9774\n",
            "Training mini-batch number 230000\n",
            "Training mini-batch number 231000\n",
            "Training mini-batch number 232000\n",
            "Training mini-batch number 233000\n",
            "Training mini-batch number 234000\n",
            "Epoch 46: validation accuracy 0.9774\n",
            "Training mini-batch number 235000\n",
            "Training mini-batch number 236000\n",
            "Training mini-batch number 237000\n",
            "Training mini-batch number 238000\n",
            "Training mini-batch number 239000\n",
            "Epoch 47: validation accuracy 0.9774\n",
            "Training mini-batch number 240000\n",
            "Training mini-batch number 241000\n",
            "Training mini-batch number 242000\n",
            "Training mini-batch number 243000\n",
            "Training mini-batch number 244000\n",
            "Epoch 48: validation accuracy 0.9774\n",
            "Training mini-batch number 245000\n",
            "Training mini-batch number 246000\n",
            "Training mini-batch number 247000\n",
            "Training mini-batch number 248000\n",
            "Training mini-batch number 249000\n",
            "Epoch 49: validation accuracy 0.9774\n",
            "Training mini-batch number 250000\n",
            "Training mini-batch number 251000\n",
            "Training mini-batch number 252000\n",
            "Training mini-batch number 253000\n",
            "Training mini-batch number 254000\n",
            "Epoch 50: validation accuracy 0.9774\n",
            "Training mini-batch number 255000\n",
            "Training mini-batch number 256000\n",
            "Training mini-batch number 257000\n",
            "Training mini-batch number 258000\n",
            "Training mini-batch number 259000\n",
            "Epoch 51: validation accuracy 0.9774\n",
            "Training mini-batch number 260000\n",
            "Training mini-batch number 261000\n",
            "Training mini-batch number 262000\n",
            "Training mini-batch number 263000\n",
            "Training mini-batch number 264000\n",
            "Epoch 52: validation accuracy 0.9776\n",
            "Training mini-batch number 265000\n",
            "Training mini-batch number 266000\n",
            "Training mini-batch number 267000\n",
            "Training mini-batch number 268000\n",
            "Training mini-batch number 269000\n",
            "Epoch 53: validation accuracy 0.9775\n",
            "Training mini-batch number 270000\n",
            "Training mini-batch number 271000\n",
            "Training mini-batch number 272000\n",
            "Training mini-batch number 273000\n",
            "Training mini-batch number 274000\n",
            "Epoch 54: validation accuracy 0.9774\n",
            "Training mini-batch number 275000\n",
            "Training mini-batch number 276000\n",
            "Training mini-batch number 277000\n",
            "Training mini-batch number 278000\n",
            "Training mini-batch number 279000\n",
            "Epoch 55: validation accuracy 0.9775\n",
            "Training mini-batch number 280000\n",
            "Training mini-batch number 281000\n",
            "Training mini-batch number 282000\n",
            "Training mini-batch number 283000\n",
            "Training mini-batch number 284000\n",
            "Epoch 56: validation accuracy 0.9774\n",
            "Training mini-batch number 285000\n",
            "Training mini-batch number 286000\n",
            "Training mini-batch number 287000\n",
            "Training mini-batch number 288000\n",
            "Training mini-batch number 289000\n",
            "Epoch 57: validation accuracy 0.9774\n",
            "Training mini-batch number 290000\n",
            "Training mini-batch number 291000\n",
            "Training mini-batch number 292000\n",
            "Training mini-batch number 293000\n",
            "Training mini-batch number 294000\n",
            "Epoch 58: validation accuracy 0.9772000000000001\n",
            "Training mini-batch number 295000\n",
            "Training mini-batch number 296000\n",
            "Training mini-batch number 297000\n",
            "Training mini-batch number 298000\n",
            "Training mini-batch number 299000\n",
            "Epoch 59: validation accuracy 0.9771000000000002\n",
            "Finished training network.\n",
            "Best validation accuracy of 97.78% obtained at iteration 194999\n",
            "Corresponding test accuracy of 97.70%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# assumes batch size of 10\n",
        "def findTroublesomeImage(net, test_data, batchsize=10):\n",
        "  batches = int(len(test_data[1].eval())/batchsize)\n",
        "  print(batches)\n",
        "  worsta = 1.0\n",
        "  worsti = 0\n",
        "  for i in range(batches):\n",
        "    outputs = net.test_mb_outputs(i)\n",
        "    for a in outputs:\n",
        "      if np.max(a)<worsta:\n",
        "        worsta = np.max(a)\n",
        "        worsti = i\n",
        "  return (worsti, worsta)"
      ],
      "metadata": {
        "id": "OSjnQIc8Tanr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(worsti, worsta) = findTroublesomeImage(net, test_data)\n",
        "print(worsti)\n",
        "print(worsta)\n",
        "\n",
        "print(test_data[1][4176].eval())\n",
        "pixeldata = test_data[0][4176].reshape((28,28)).eval()\n",
        "pixeldata = [[round(p*255) for p in pix] for pix in pixeldata]\n",
        "print(pixeldata)\n",
        "plt.figure()\n",
        "plt.imshow(pixeldata, cmap=\"gray_r\")\n",
        "net.test_mb_outputs(417)\n"
      ],
      "metadata": {
        "id": "5gWTjX21Hc_r",
        "outputId": "b2962ab3-7a2e-4c4c-a799-0fb141124a98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 893
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "170\n",
            "0.2988699\n",
            "2\n",
            "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 19, 39, 28, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 190, 207, 146, 97, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 198, 252, 161, 247, 117, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 102, 253, 253, 253, 244, 193, 35, 5, 39, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 46, 224, 253, 253, 253, 253, 215, 152, 253, 218, 138, 72, 0, 0, 0, 0, 0, 60, 111, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 40, 168, 251, 253, 253, 253, 253, 253, 253, 253, 129, 67, 240, 200, 227, 240, 245, 147, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 67, 185, 222, 233, 253, 253, 253, 253, 144, 50, 174, 69, 221, 253, 253, 46, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 123, 190, 213, 123, 123, 158, 179, 253, 253, 254, 253, 46, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 16, 0, 0, 70, 253, 253, 253, 253, 154, 4, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 24, 87, 252, 230, 250, 234, 44, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 38, 214, 253, 253, 246, 125, 234, 92, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 121, 253, 253, 253, 104, 131, 253, 92, 0, 58, 70, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 89, 246, 253, 253, 253, 104, 130, 228, 195, 82, 239, 244, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 72, 131, 253, 253, 253, 253, 253, 190, 233, 254, 253, 229, 55, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 143, 236, 253, 253, 253, 253, 212, 122, 131, 38, 38, 125, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 31, 250, 251, 246, 170, 92, 31, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 76, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.39680437e-14, 9.99985278e-01, 1.39406339e-07, 1.54249983e-06,\n",
              "        5.49200649e-07, 8.92306502e-07, 6.79408686e-07, 1.17282082e-06,\n",
              "        9.72172347e-06, 3.91708799e-09],\n",
              "       [5.62648615e-14, 9.99672830e-01, 3.01405798e-08, 7.49568653e-06,\n",
              "        2.03889741e-07, 8.40256362e-08, 7.78001834e-07, 6.11664564e-06,\n",
              "        3.12009040e-04, 4.35986834e-07],\n",
              "       [1.86319254e-15, 1.51051237e-11, 2.79596072e-14, 1.11993685e-10,\n",
              "        9.99999940e-01, 1.70915071e-09, 5.71029639e-12, 3.56250029e-10,\n",
              "        1.25378963e-09, 6.39979305e-08],\n",
              "       [2.71970606e-08, 3.99346281e-05, 9.15258169e-01, 6.04606932e-03,\n",
              "        7.85799846e-02, 2.44728511e-07, 4.50197622e-05, 6.08526785e-09,\n",
              "        3.03579527e-05, 2.05887986e-07],\n",
              "       [1.94683584e-13, 6.24188201e-11, 6.06522319e-13, 1.21527091e-08,\n",
              "        5.33122184e-05, 1.32223343e-10, 5.38818946e-12, 6.82835677e-10,\n",
              "        1.09107781e-07, 9.99946594e-01],\n",
              "       [4.77961551e-12, 1.94295104e-08, 6.17293661e-10, 1.67620374e-06,\n",
              "        1.74039047e-13, 2.13535647e-11, 1.52583655e-16, 9.99993145e-01,\n",
              "        7.95555705e-11, 5.13096938e-06],\n",
              "       [5.79121862e-09, 1.44541247e-07, 5.12736551e-05, 7.13800637e-06,\n",
              "        2.49428085e-05, 1.04638559e-06, 1.96381705e-03, 9.97654438e-01,\n",
              "        2.97155697e-04, 2.22460432e-08],\n",
              "       [8.30493008e-09, 2.60333479e-07, 7.18059354e-08, 1.85792798e-10,\n",
              "        3.62447958e-04, 9.95875478e-01, 3.75796249e-03, 2.99484678e-12,\n",
              "        2.50394514e-08, 3.77000333e-06],\n",
              "       [1.76834681e-06, 9.94137645e-01, 3.95306532e-04, 2.17337965e-05,\n",
              "        2.10997787e-05, 3.46029946e-03, 9.47421533e-04, 9.12073460e-07,\n",
              "        1.01330341e-03, 5.14244391e-07],\n",
              "       [3.93952923e-14, 9.99982178e-01, 1.15312115e-08, 4.11861129e-06,\n",
              "        1.49914783e-08, 4.94290120e-09, 4.13644621e-08, 1.59452622e-06,\n",
              "        6.73713566e-06, 5.29963472e-06]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOF0lEQVR4nO3df6xU9ZnH8c+jYlRoIiw3eBWVbjFRYlxKJmiCVFYj4o/kUmNMiamuYm6DmkDSuGu6f2D8C3ftktVsam63WDRdSiOQamIES5ooGhsHwyLgD1gCFuTHRYiAinjl2T/uobnine9c5pyZM9zn/UomM3Oe+c55MuHDmXu+M/M1dxeA4e+sshsA0BqEHQiCsANBEHYgCMIOBHFOK3c2duxYnzBhQit3CYSyY8cOHThwwAar5Qq7mc2S9J+Szpb03+6+KPX4CRMmqFqt5tklgIRKpVKz1vDbeDM7W9J/SbpV0iRJc8xsUqPPB6C58vzNPlXSNnff7u7HJf1eUlcxbQEoWp6wXyLprwPu78q2fYuZdZtZ1cyqvb29OXYHII+mn4139x53r7h7paOjo9m7A1BDnrDvlnTpgPvjs20A2lCesL8j6Qoz+76ZnSvpJ5JeKqYtAEVreOrN3fvM7BFJq9U/9bbE3TcX1hmAQuWaZ3f3VyS9UlAvAJqIj8sCQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgsi1ZLOZ7ZB0RNI3kvrcvVJEUwCKlyvsmX909wMFPA+AJuJtPBBE3rC7pDVmtt7Mugd7gJl1m1nVzKq9vb05dwegUXnDfr27T5F0q6SHzexHpz7A3XvcveLulY6Ojpy7A9CoXGF3993Z9X5JqyRNLaIpAMVrOOxmNtLMvnfytqSZkjYV1RiAYuU5Gz9O0iozO/k8/+PurxbSVRN8+umnyfqBA+kJhZEjR9asXXzxxcmxZ53FeVCUr+Gwu/t2Sf9QYC8AmohDDhAEYQeCIOxAEIQdCIKwA0EU8UWYM8Jdd92VrB86dChZnz59es3alClTkmPvv//+ZB1oBY7sQBCEHQiCsANBEHYgCMIOBEHYgSAIOxBEmHn2Y8eOJesbN25M1q+55pqatXnz5iXH1vv67KOPPpqsA0XgyA4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQYSZZ6/3nfK33347WX/hhRca3vfChQuT9ZdffjlZf+6555L1yy67rGZt+/btybGpn8iWpPHjxyfrOHNwZAeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIMLMs1933XXJemquWpI+/vjjhvf95ZdfJutvvPFGsn777bcn6zfccEPNWk9PT3LsxIkTk/XZs2cn6zfddFOyPmvWrGS9LOvWrUvW169fn6zPnz+/yHZaou6R3cyWmNl+M9s0YNsYM3vNzLZm16Ob2yaAvIbyNv63kk797/kxSWvd/QpJa7P7ANpY3bC7++uSDp6yuUvS0uz2Uknp93oAStfoCbpx7r4nu71X0rhaDzSzbjOrmlm1t7e3wd0ByCv32Xh3d0meqPe4e8XdKx0dHXl3B6BBjYZ9n5l1SlJ2vb+4lgA0Q6Nhf0nSfdnt+yT9sZh2ADRL3Xl2M1smaYaksWa2S9JCSYsk/cHM5kraKenuZjZZhNTvvkvS6tWrk/XUd9JffPHF5NgTJ04k6/V8+OGHueop27ZtS9afeuqpZP38889P1keMGFGzdueddybHXnnllcn67t27k/UjR47UrB0/fjw5du7cucn6mahu2N19To1S+tMUANoKH5cFgiDsQBCEHQiCsANBEHYgiDBfca2n3jTP8uXLa9bWrl2bHPvkk08m61u3bk3WP//882S9zI8hf/bZZ8n66NG1vxB54403Jsc+9NBDyXp3d3eyfvjw4WQ9z3OfiTiyA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQzLMXoN7PKder17Nly5ZkfeXKlTVr9T4DsHfv3mT9nnvuSdbreeaZZ2rWvv766+TYmTNn5to3vo0jOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwTz7GWDSpEkN1xcsWJAc29fXl6xfeOGFyXo9a9asqVm75ZZbcj13Hg8++GCyXu81PxNxZAeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIJhnH+ZGjRqVa/yhQ4eS9a+++qrh8WaWHDtu3Lhk/cCBA8l66jMC8+fPT44955zhF426R3YzW2Jm+81s04Btj5vZbjPbkF1ua26bAPIaytv430qaNcj2xe4+Obu8UmxbAIpWN+zu/rqkgy3oBUAT5TlB94iZbcze5tdc0MvMus2sambVMtckA6JrNOy/kvQDSZMl7ZH0y1oPdPced6+4e6Wjo6PB3QHIq6Gwu/s+d//G3U9I+rWkqcW2BaBoDYXdzDoH3P2xpE21HgugPdSdTDSzZZJmSBprZrskLZQ0w8wmS3JJOyT9rIk9ook++uijZL2rqytZ/+CDDxre93nnnZes11uf/emnn07WFy9eXLN29dVXJ8cOR3XD7u5zBtn8myb0AqCJ+LgsEARhB4Ig7EAQhB0IgrADQQy/7/HhtKxYsSJZzzO1Vk9qakySdu3alazX+4prHm+99VayfvTo0WS9HZeb5sgOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewzz7MLVu2LFl/4oknWtTJ6av3U9OdnZ3J+rPPPluzdsEFFyTHPvDAA8n68ePHk/UvvvgiWS8DR3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIJ59mHg1VdfrVnbvHlzcuyxY8eKbmfI5s2bl6zPmDEjWe/r60vW33zzzYZqknTRRRcl66tXr07W2xFHdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0Ignn2YSA1z/7888+3sJPTc9VVVyXrN998c7I+bdq0ZP3ee+897Z5OWr58ebJ+7bXXNvzcZal7ZDezS83sz2a2xcw2m9n8bPsYM3vNzLZm16Ob3y6ARg3lbXyfpJ+7+yRJ10l62MwmSXpM0lp3v0LS2uw+gDZVN+zuvsfd381uH5H0vqRLJHVJWpo9bKmk2c1qEkB+p3WCzswmSPqhpL9IGufue7LSXknjaozpNrOqmVV7e3tztAogjyGH3cxGSVohaYG7Hx5Yc3eX5IONc/ced6+4e6WjoyNXswAaN6Swm9kI9Qf9d+6+Mtu8z8w6s3qnpP3NaRFAEaz/oJx4QP/v+S6VdNDdFwzY/u+SPnX3RWb2mKQx7v7PqeeqVCperVYLaBsD7dy5s2Zt4sSJybEnTpxI1s8999xk/Y477kjWu7q6atamT5+eHHv55Zcn6/iuSqWiarU66G9wD2WefZqkn0p6z8w2ZNt+IWmRpD+Y2VxJOyXdXUSzAJqjbtjdfZ2kWr/Wf1Ox7QBoFj4uCwRB2IEgCDsQBGEHgiDsQBB8xXUYSM1Hr1q1Kjn2k08+Sda7u7sb6gnthyM7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTBPPswV+/75oiDIzsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EUTfsZnapmf3ZzLaY2WYzm59tf9zMdpvZhuxyW/PbBdCoofx4RZ+kn7v7u2b2PUnrzey1rLbY3Z9qXnsAijKU9dn3SNqT3T5iZu9LuqTZjQEo1mn9zW5mEyT9UNJfsk2PmNlGM1tiZqNrjOk2s6qZVXt7e3M1C6BxQw67mY2StELSAnc/LOlXkn4gabL6j/y/HGycu/e4e8XdKx0dHQW0DKARQwq7mY1Qf9B/5+4rJcnd97n7N+5+QtKvJU1tXpsA8hrK2XiT9BtJ77v7fwzY3jngYT+WtKn49gAUZShn46dJ+qmk98xsQ7btF5LmmNlkSS5ph6SfNaVDAIUYytn4dZJskNIrxbcDoFn4BB0QBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIc/fW7cysV9LOAZvGSjrQsgZOT7v21q59SfTWqCJ7u9zdB/39t5aG/Ts7N6u6e6W0BhLatbd27Uuit0a1qjfexgNBEHYgiLLD3lPy/lPatbd27Uuit0a1pLdS/2YH0DplH9kBtAhhB4IoJexmNsvMPjSzbWb2WBk91GJmO8zsvWwZ6mrJvSwxs/1mtmnAtjFm9pqZbc2uB11jr6Te2mIZ78Qy46W+dmUvf97yv9nN7GxJH0m6WdIuSe9ImuPuW1raSA1mtkNSxd1L/wCGmf1I0lFJz7v71dm2f5N00N0XZf9Rjnb3f2mT3h6XdLTsZbyz1Yo6By4zLmm2pH9Sia9doq+71YLXrYwj+1RJ29x9u7sfl/R7SV0l9NH23P11SQdP2dwlaWl2e6n6/7G0XI3e2oK773H3d7PbRySdXGa81Ncu0VdLlBH2SyT9dcD9XWqv9d5d0hozW29m3WU3M4hx7r4nu71X0rgymxlE3WW8W+mUZcbb5rVrZPnzvDhB913Xu/sUSbdKejh7u9qWvP9vsHaaOx3SMt6tMsgy439T5mvX6PLneZUR9t2SLh1wf3y2rS24++7ser+kVWq/paj3nVxBN7veX3I/f9NOy3gPtsy42uC1K3P58zLC/o6kK8zs+2Z2rqSfSHqphD6+w8xGZidOZGYjJc1U+y1F/ZKk+7Lb90n6Y4m9fEu7LONda5lxlfzalb78ubu3/CLpNvWfkf8/Sf9aRg81+vp7Sf+bXTaX3ZukZep/W/e1+s9tzJX0d5LWStoq6U+SxrRRby9Iek/SRvUHq7Ok3q5X/1v0jZI2ZJfbyn7tEn215HXj47JAEJygA4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEg/h96kEuDYB7syQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show theano"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzPpIsXuF6Eb",
        "outputId": "99b67011-4f1d-419f-8827-13c5dbb6eb10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Theano\n",
            "Version: 0.8.0\n",
            "Summary: Optimizing compiler for evaluating mathematical expressions on CPUs and GPUs.\n",
            "Home-page: http://deeplearning.net/software/theano/\n",
            "Author: LISA laboratory, University of Montreal\n",
            "Author-email: theano-dev@googlegroups.com\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.8/dist-packages\n",
            "Requires: numpy, scipy, six\n",
            "Required-by: \n"
          ]
        }
      ]
    }
  ]
}